Project: Redseer Financial Analyst Agent (The "Table-Aware" RAG)
1. Problem Statement
The "Table Problem": Redseer Strategy Consultants need to benchmark companies (e.g., "Compare AWS vs. Azure Revenue").

Current Failure: Standard RAG pipelines chunk documents into 500-token pieces. This cuts financial tables in half, causing the AI to hallucinate numbers or lose column alignment (e.g., mixing up 2023 and 2024 data).

Our Solution: A "Structure-Aware" Ingestion Pipeline that preserves HTML tables, converts them to Markdown grids, and uses Long-Context Retrieval (Gemini 2.5) to analyze entire Annual Reports without chunking the tables.

2. Architecture & Workflow
Phase 1: Ingestion (The "Gold Mine" Strategy)
We do not use PDFs. We use the raw SEC full-submission.txt to access the source HTML.

Source: Download 10-K filings via sec-edgar-downloader.

Cleaning: Custom Regex Parser extracts the main 10-K HTML document, discarding XML/Images.

Transformation:

Text: Kept as normal.

Tables: Detected via <table> tags and converted to Markdown (| Col A | Col B |) using markdownify.

Storage:

Artifacts: Full Markdown files (AMZN_2024.md) stored in Local/S3.

Vector DB (Qdrant): Stores Metadata (Ticker, Year, Sector) and Summaries for search.

Phase 2: Retrieval (The "Hybrid" Approach)
User Query: "Compare AWS and Azure revenue."

Step A (Router): LLM identifies target companies: ['AMZN', 'MSFT'].

Step B (Selection): Vector DB filters for the latest Annual Report for these 2 companies.

Result: Returns file paths for AMZN_2024.md and MSFT_2024.md.

Step C (Long-Context Generation):

We load the Full Content of these 2 specific files (approx. 100k - 200k tokens) into Gemini 2.5 Flash.

Why: Gemini has a 1M+ token window. Reading the full file guarantees we don't miss "Notes to Financial Statements" which are often 50 pages away from the main table.

3. Current Progress & Data Status
Data Source: SEC EDGAR Database.

Current Dataset: 89 Companies (Top US Tech/Finance) downloaded via Colab.

Volume: ~420MB of raw text.

Format: Zipped folder Redseer_100_Dataset.zip.

Proof of Concept (PoC):

Successfully parsed Amazon's 10-K.

Successfully extracted the "Segment Information" table (AWS vs International).

Successfully used Gemini 2.5 Flash to generate a comparative insight.

4. Technical Stack (For Cursor Setup)
Backend:

Language: Python 3.10+

Framework: FastAPI (to serve the Agent)

LLM Orchestration: LangChain / LangGraph

Models:

Reasoning: gemini-2.5-flash (or gemini-1.5-pro if available).

Embeddings: text-embedding-3-small (OpenAI) or jina-embeddings-v3 (Open Source).

Database:

Vector: Qdrant (Docker local).

Document: Local File System (for now).

Libraries to Install:

Bash

pip install fastapi uvicorn langchain langchain-google-genai langchain-community qdrant-client sec-edgar-downloader markdownify unstructured
5. Implementation Roadmap (Next Steps in Cursor)
Step 1: Local Ingestion Script (ingest.py)
Goal: Process the downloaded 89 companies.

Logic:

Unzip Redseer_100_Dataset.zip.

Iterate through every folder.

Run the extract_largest_html function (from our Colab prototype).

Run markdownify on the HTML.

Save the clean .md file to a /processed_data folder.

Step 2: The Vector Indexer (index.py)
Goal: Make the files searchable.

Logic:

Initialize Qdrant Client.

For each .md file, create a PointStruct:

Python

{
    "id": uuid,
    "vector": embedding_model.encode(company_summary),
    "payload": {
        "ticker": "AMZN",
        "year": "2024",
        "file_path": "./processed_data/AMZN_2024.md"
    }
}
Upload to Qdrant.

Step 3: The Agent API (server.py)
Goal: The Brain.

Logic:

Endpoint POST /analyze receives { "query": "Compare X and Y" }.

Router Node: Extracts tickers ['X', 'Y'].

Retriever Node: Queries Qdrant for file_path of X and Y.

Loader Node: Reads the full content of those 2 files.

Generator Node: Sends (Query + File_X_Content + File_Y_Content) to Gemini.

Returns JSON response.

6. Key Differentiators (The "Pitch" to Redseer)
Zero-Chunking on Tables: We treat tables as atomic units, preventing data loss.

Visual Structure Preservation: Using HTML parsing instead of PDF scraping keeps the row/column alignment intact.

Agentic "Self-Correction": If the Agent can't find a table, it doesn't hallucinate; it scans the "Notes" section or reports "Data Not Found" (Trust Layer).

Exa.ai Integration: (Planned) To auto-fetch new reports without manual upload.

7. Immediate Action Items
Unzip the dataset on local machine.

Create ingest.py to clean the raw files.

Build the basic Qdrant index.